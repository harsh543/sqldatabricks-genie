{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dea35966-0b7a-4520-b371-a1c321be4a02",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# \\[Part 1 - Admin POV] Databricks SQL in Action: Intelligent Data Warehousing, Analytics, and BI Hands-On Lab\n",
    "\n",
    "Welcome to the **Administrator POV** section of the Databricks SQL Hands-On Lab!\n",
    "\n",
    "## Objectives\n",
    "\n",
    "By the end of this section, you will:\n",
    "\n",
    "* Understand the responsibilities of a Databricks Workspace Administrator.\n",
    "* Successfully configure a serverless SQL Warehouse.\n",
    "* Efficiently manage data assets using catalogs, schemas, and volumes.\n",
    "* Create streaming tables and optimize queries using materialized views.\n",
    "* Implement primary and foreign key constraints for data integrity.\n",
    "* Monitor and optimize SQL queries through query history and profiling.\n",
    "* Leverage AI to auto-generate documentation of your data assets.\n",
    "* Manage user access through effective permission settings.\n",
    "\n",
    "## Target Audience\n",
    "\n",
    "This guide is tailored specifically for roles responsible for data governance, management, and administration, including:\n",
    "\n",
    "* Data Architects\n",
    "* Data Warehousing Architects\n",
    "* Database Administrators\n",
    "* Databricks Workspace Administrators\n",
    "\n",
    "## Lab Outline\n",
    "\n",
    "We will use a sample dataset containing airline and airport data to demonstrate Databricks SQL’s capabilities from an administrative perspective. The following sections detail each step:\n",
    "\n",
    "* **Intro:** Navigating Databricks SQL as a Workspace Administrator\n",
    "* **Step 0:** Configure Serverless SQL Warehouse\n",
    "* **Step 1:** Understanding your default catalog\n",
    "* **Step 2:** Create a schema/database and define schema/database name\n",
    "* **Step 3:** Create a volume\n",
    "* **Step 4:** Create streaming tables\n",
    "* **Step 5:** Create materialized views\n",
    "* **Step 6:** View query history and query profiles\n",
    "* **Step 7:** Generate AI-based documentation for data assets\n",
    "* **Step 8:** Set permissions on data assets\n",
    "\n",
    "## Important Note\n",
    "\n",
    "This notebook contains SQL queries that we will copy and execute in the Databricks SQL Editor. **We will NOT execute any queries directly within this notebook!**\n",
    "\n",
    "## Let's Get Started\n",
    "\n",
    "First, we'll dive into the essential role of the Databricks Workspace Administrator and explore the tools and strategies to effectively manage your data using Databricks SQL!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4550d8c3-b2e2-4308-bdb6-7e2a2f780067",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Intro: Navigating Databricks SQL as a Workspace Administrator\n",
    "\n",
    "Once you’re logged into the workspace, Databricks presents a landing page with unified navigation.\n",
    "\n",
    "The Databricks workspace offers a unified navigation experience, providing easy access to various data engineering, data science, and analytics tools. On the left-side navigation pane, users can find a comprehensive list of features, with a notable focus on SQL capabilities. The SQL section is called **Databricks SQL** and it is a complete data warehouse that includes:\n",
    "\n",
    "* SQL Editor: create and run SQL queries\n",
    "* Queries: monitor and organize saved and historical queries\n",
    "* Dashboards: visualize data and build shareable dashboards\n",
    "* Genie (natural language querying): interact with data using natural language\n",
    "* Alerts: create custom alerts based on query thresholds\n",
    "* Query History: review performance, duration, and resource usage\n",
    "* SQL Warehouses: compute resources that let's you query and explore data on Databricks\n",
    "\n",
    "The interface emphasizes user-friendly navigation, allowing data engineers, data scientists, data analysts, and other data professionals to seamlessly switch between different aspects of their data projects within a single, cohesive environment.\n",
    "\n",
    "\n",
    "![Databricks SQL Navigation Screenshot](https://data-ai-lakehouse.cloud.databricks.com//files/tables/images/Screenshot_2025_06_04_at_11_26_36_AM.png)\n",
    "\n",
    "### Pre-Lab Setup Checklist\n",
    "\n",
    "* [ ] You have a Databricks account with necessary user privileges (you are NOT an admin in this workspace)\n",
    "* [ ] You can access the Databricks SQL section from the navigation panel (already enabled prior to workshop)\n",
    "* [ ] Your workspace has Unity Catalog enabled (already enabled prior to workshop)\n",
    "\n",
    "> **Note for First-Time Users:** Spend a few minutes exploring the layout. Understanding where each SQL tool lives will make this lab much smoother.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c66d48ec-010a-4876-87ef-9679aa0609e9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 0: Configure SQL Warehouse\n",
    "\n",
    "As the admin of a Databricks workspace, you are responsible for configuring and monitoring SQL Warehouses. These are the compute resources that execute SQL queries within Databricks SQL.\n",
    "\n",
    "Follow these steps to inspect or create a SQL Warehouse:\n",
    "\n",
    "1. From the left-hand navigation pane, click on **SQL Warehouses**\n",
    "2. You may see a pre-configured SQL Warehouse that is already running. If it is not, click the **play icon** to start it\n",
    "3. Click on the warehouse name to review its configuration. You may not be able to edit it in this lab, and that's okay, your instructor will demonstrate these steps\n",
    "4. If you have permission to edit or create a warehouse, click **Edit** or **Create Warehouse** and configure the following settings:\n",
    "\n",
    "### SQL Warehouse Settings Explained\n",
    "\n",
    "* **Name**: Choose something descriptive like `Serverless Starter Warehouse`\n",
    "* **Cluster Size**: We recommend starting with an **extra small** cluster. Larger clusters offer more parallelism and faster response time\n",
    "* **Auto Stop**: Defines how many idle minutes should pass before the warehouse automatically shuts down to save costs\n",
    "* **Scaling**: This handles **horizontal scaling**. By default, a warehouse can handle up to 10 concurrent queries. You can enable auto-scaling to support more users without queueing\n",
    "* **Serverless**: Opt for serverless to get auto-managed compute with faster startup times and automatic scaling\n",
    "* **Tags**: Add metadata tags (key-value pairs) to track usage and cost by team, project, or department\n",
    "* **Unity Catalog**: Enable Unity Catalog to manage permissions at the catalog, schema, table, and view levels. It also enables fine-grained audit logging\n",
    "\n",
    "> **Reminder:** In this lab, your permissions are limited. You may not be able to edit warehouses directly, but these steps are included so you can understand how admins configure compute environments in production settings\n",
    "\n",
    "### Warehouse Permissions\n",
    "\n",
    "SQL Warehouses have four permission levels:\n",
    "\n",
    "* **No Permissions**\n",
    "* **Can Monitor**\n",
    "* **Can Use**\n",
    "* **Can Manage**\n",
    "\n",
    "Your ability to interact with warehouses depends on your assigned role. For this lab, not all users will have full permissions and that’s expected\n",
    "\n",
    "### Connection Details\n",
    "\n",
    "The **Connection Details** tab provides credentials and tokens for integrating Databricks SQL with participating apps, tools, SDK, or API\n",
    "\n",
    "* When needed, you can generate a **personal access token** for secure connections and then...\n",
    "* configure connection URLs and ports as needed\n",
    "\n",
    "### Partner Connect\n",
    "\n",
    "**Partner Connect** allows you to create trial accounts and connect your Databricks environment to partner tools\n",
    "\n",
    "* This lab won’t use Partner Connect, but it’s helpful for future integrations with ETL, ML, and BI platforms.\n",
    "\n",
    "### Monitoring\n",
    "\n",
    "The **Monitoring** tab helps you:\n",
    "\n",
    "* Track the number of active queries\n",
    "* See how many clusters are allocated\n",
    "* Review performance metrics in real-time\n",
    "\n",
    "Now that your SQL Warehouse (compute layer) is configured, you're ready to start ingesting and managing data in your lakehouse!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "eca69c63-7b47-4499-82bf-3935d53af292",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "### Step 1: Understanding Your Default Catalog\n",
    "\n",
    "In Unity Catalog, a **catalog** is the topmost container in the data governance hierarchy. It’s used to logically organize your data across workspaces and manage access control. Catalogs hold schemas, which in turn contain tables, views, functions, and volumes.\n",
    "\n",
    "By default, a catalog has already been created for you. It’s named after your lab or workspace name and is ready to use. You do **not** need to create a new catalog manually.\n",
    "\n",
    "However, if you ever need to create an additional catalog (e.g., for testing or separation of environments), here’s how you could do that using the Catalog Explorer UI:\n",
    "\n",
    "1. In the **Data** tab on the sidebar, open **Catalog Explorer**\n",
    "2. Click the **Create Catalog** button in the top-right corner\n",
    "3. Enter a name (e.g., `sandbox_catalog`) and optional comment or owner assignment\n",
    "4. Click **Create**\n",
    "\n",
    "For most use cases, your default catalog will suffice, especially for labs and exercises using Genie. Now, we’ll move to the next layer—schemas (or databases), which live inside the catalog and further organize your data assets.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c5caca63-977c-4b8f-8ee9-983c87cff8d4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "### Step 2: Understanding Your Default Schema\n",
    "\n",
    "In Unity Catalog, a **schema** (also known as a **database**) is the second layer in the data object hierarchy. Schemas reside within catalogs and are used to organize tables, views, functions, and other data objects.\n",
    "\n",
    "Within your default catalog, a schema named **`default`** has already been created for you. You do **not** need to create a schema manually to complete most tasks in Genie.\n",
    "\n",
    "However, if you ever need a new schema for organizational or testing purposes, here is how you can create one using the Catalog Explorer:\n",
    "\n",
    "1. Open the **Data** tab from the sidebar\n",
    "2. Navigate to your catalog\n",
    "3. Click the **Create Schema** button in the top right\n",
    "4. Provide a name (e.g., `analysis_schema`) and optional description\n",
    "5. Click **Create**\n",
    "\n",
    "Unless you have a specific use case, the `default` schema will meet all your data organization needs while working in this lab.\n",
    "\n",
    "With your catalog and schema now established, you're ready to start creating storage containers and loading data!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d726f9d8-e576-4f43-8bf3-e3c67ec15d12",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Step 3: Use an Existing Volume for File Storage\n",
    "\n",
    "In Unity Catalog, a **volume** is designed to provide governance over **non-tabular datasets**, such as raw files in structured, semi-structured, or unstructured formats. Volumes act as logical containers for data stored in underlying cloud object storage like AWS S3 or Azure Blob Storage.\n",
    "\n",
    "> **Important Context for This Lab:**\n",
    "> In a real-world production environment, customers typically ingest or stream data into Databricks from their own cloud storage (e.g., S3 buckets or ADLS). However, to simplify setup and ensure consistent access for all participants—especially since not all users have cloud credentials or admin privileges—we have **pre-created a shared volume** that you’ll use throughout this workshop\n",
    "\n",
    "> **Note:** While we are using this shared volume as a substitute for raw data ingestion, volumes are *not* typically used this way in production. This is just to help simulate the experience in a shared lab environment\n",
    "\n",
    "> **Important Limitation:** You **cannot** use volumes as a location for tables. Volumes are intended for **path-based data access only**. If you want to work with tabular data using Unity Catalog’s capabilities, you should use **tables** instead. We will convert these files\n",
    "\n",
    "### What You'll Learn\n",
    "\n",
    "* What volumes are used for in Unity Catalog\n",
    "* The types of data volumes can support\n",
    "* Why volumes are different from tables\n",
    "* How to navigate to a pre-configured volume and upload files\n",
    "\n",
    "### Volume Purpose Recap\n",
    "\n",
    "Volumes are often used to:\n",
    "\n",
    "* Store files in formats like CSV, JSON, Parquet, Avro, images, and more\n",
    "* Manage data access and auditing using Unity Catalog’s governance features\n",
    "* Serve as landing zones for unstructured or semi-structured files before transformation\n",
    "\n",
    "### Using the Pre-Created Volume\n",
    "\n",
    "1. Navigate to the shared volume already created for you by your instructor. The instructor will display the full path.\n",
    "\n",
    "3. You can use my volume in the next step by simply copying and pasting this\n",
    "\n",
    "```sql\n",
    "/Volumes/adminuser3795939/default/raw_airline_data/airports/\n",
    "/Volumes/adminuser3795939/default/raw_airline_data/flights/\n",
    "/Volumes/adminuser3795939/default/raw_airline_data/lookupcodes/\n",
    "```\n",
    "\n",
    "> If you’d like to learn how to create your own volume using SQL, you can explore the official documentation here: [Create a Volume](https://docs.databricks.com/en/volumes/create-volume.html).\n",
    "\n",
    "### Summary\n",
    "\n",
    "Now that the data files are uploaded into your shared volume, you’ll be able to reference them in future steps as you build streaming tables, materialized views, and more.\n",
    "\n",
    "Let’s begin working with these files to extract, transform, and load your data using SQL!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4373b156-dfa7-4855-86fc-b806e1b01c4f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Step 4: Create Streaming Tables\n",
    "\n",
    "[Streaming tables](https://docs.databricks.com/en/sql/language-manual/sql-ref-syntax-ddl-create-streaming-table.html#create-streaming-table) are stateful tables designed to continuously ingest and process new data. Because many datasets grow over time, streaming tables are ideal for real-time or near-real-time ingestion use cases.\n",
    "\n",
    "Streaming tables:\n",
    "\n",
    "* Ensure each row is processed once\n",
    "* Support incremental and continuous computation\n",
    "* Are optimized for freshness and low-latency pipelines\n",
    "\n",
    "> **Note:** In production, streaming tables usually source data from message queues or cloud object storage. For this lab, we’ll simulate streaming using the static files you uploaded to your shared volume\n",
    "\n",
    "We’ll organize data following the [Medallion Lakehouse Architecture](https://docs.databricks.com/en/lakehouse/medallion.html#what-is-the-medallion-lakehouse-architecture):\n",
    "\n",
    "* **Bronze:** Raw ingested data\n",
    "* **Silver:** Cleansed and conformed data\n",
    "* **Gold:** Curated business-level data\n",
    "\n",
    "We’ll start by creating Bronze streaming tables using the CSVs in your volume.\n",
    "\n",
    "### Instructions\n",
    "\n",
    "1. Open the SQL Editor\n",
    "2. **Create a new query** and save it with the appropriate title (e.g., `Bronze_ST`)\n",
    "3. Copy and paste the following code snippets into the SQL Editor, replacing `<catalog>` and `<schema>` with the names you created in earlier steps\n",
    "4. In the SQL editor, there is a catalog and schema selector. Go ahead and set it to the catalog and the schema that you created\n",
    "5. Run each snippet of code 1 at a time?\n",
    "\n",
    "### Streaming Table: Airports\n",
    "\n",
    "```sql\n",
    "-- Create airports streaming table\n",
    "CREATE OR REFRESH STREAMING TABLE airports_bronze_st AS\n",
    "SELECT * FROM STREAM read_files('/Volumes/adminuser3795939/default/raw_airline_data/airports/');\n",
    "```\n",
    "\n",
    "### Streaming Table: Flights\n",
    "\n",
    "```sql\n",
    "-- create flights streaming table\n",
    "CREATE OR REFRESH STREAMING TABLE flights_bronze_st AS\n",
    "SELECT * FROM STREAM read_files('/Volumes/adminuser3795939/default/raw_airline_data/flights/');\n",
    "```\n",
    "\n",
    "### Streaming Table: Lookupcodes\n",
    "\n",
    "```sql\n",
    "== create lookup codes streaming table\n",
    "CREATE OR REFRESH STREAMING TABLE lookupcodes_bronze_st AS\n",
    "SELECT * FROM STREAM read_files('/Volumes/adminuser3795939/default/raw_airline_data/lookupcodes');\n",
    "```\n",
    "\n",
    "Once these tables are created, you’ll have foundational raw data to begin cleansing and transforming in the next steps of the lab.\n",
    "\n",
    "---\n",
    "\n",
    "### Other Ways to Ingest Data into Databricks\n",
    "\n",
    "Databricks supports a variety of ingestion methods depending on your use case and scale. While we're using volumes here for simplicity, these are the more robust ways to bring data into Databricks in production:\n",
    "\n",
    "* **Add Data UI**: A guided interface for uploading files and connecting to cloud storage\n",
    "* **SQL Editor or Notebooks**: Manual or scheduled ingestion using SQL or Python\n",
    "* **Auto Loader**: Efficient, scalable ingestion of new files from cloud storage with minimal setup\n",
    "* **COPY INTO**: Ideal for SQL users needing idempotent and incremental ingestion from cloud storage\n",
    "* **Delta Live Tables (DLT)**: Declarative pipelines with built-in monitoring and data quality checks\n",
    "* **LakeFlow Connect**: Fully managed connectors for enterprise applications, cloud storage, and databases\n",
    "* **Structured Streaming**: Real-time ingestion from sources like Kafka or Kinesis\n",
    "\n",
    "> Databricks recommends using **streaming tables** for incremental ingestion via SQL. They combine ease of use with powerful, production-grade performance.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ea98e23d-6629-40e6-a6d9-9b1fe053b83c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 5: Create Materialized Views\n",
    "\n",
    "[Materialized views](https://docs.databricks.com/en/views/materialized.html#use-materialized-views-in-databricks-sql) provide a powerful way to process and transform data efficiently in Databricks. They are useful for compliance, corrections, aggregations, and general change data capture (CDC).\n",
    "\n",
    "Materialized views reduce cost and improve query performance by pre-computing expensive or frequently executed logic. They also make it easy to clean, enrich, and denormalize raw or bronze-layer tables for downstream analytics.\n",
    "\n",
    "In this step, we’ll create views for the **Silver** and **Gold** layers of the [Medallion Lakehouse Architecture](https://docs.databricks.com/en/lakehouse/medallion.html#what-is-the-medallion-lakehouse-architecture):\n",
    "\n",
    "* **Silver Layer:** Validated and enriched data\n",
    "* **Gold Layer:** Aggregated and business-ready data\n",
    "\n",
    "> **Note:** Your Silver layer could also consist of streaming tables, depending on your data pipeline needs.\n",
    "\n",
    "---\n",
    "\n",
    "### Materialized View: Airports Silver Layer\n",
    "\n",
    "1. **Open the SQL Editor**\n",
    "2. **Create a new query** and name it `Airports Silver_MV`\n",
    "3. **Paste the following SQL** and make sure the catalog and schema selector are the catalog and schema that you created\n",
    "\n",
    "```sql\n",
    "\n",
    "CREATE OR REPLACE MATERIALIZED VIEW airports_silver_mv\n",
    "(\n",
    "  IATA STRING,\n",
    "  City STRING,\n",
    "  State STRING,\n",
    "  CONSTRAINT pk_airports PRIMARY KEY (IATA)\n",
    ") AS\n",
    "SELECT IATA, City, State\n",
    "FROM airports_bronze_st;\n",
    "```\n",
    "\n",
    "This view extracts and cleans specific columns from the Bronze layer for analysis while incorporating some data modeling with the addition of a primary key\n",
    "\n",
    "---\n",
    "\n",
    "### Materialized View: Lookupcodes Silver Layer\n",
    "\n",
    "1. **Create a new query** and name it `Lookupcodes Silver_MV`.\n",
    "2. **Paste the following SQL** and make sure the catalog and schema selector are the catalog and schema that you created\n",
    "\n",
    "```sql\n",
    "\n",
    "CREATE OR REPLACE MATERIALIZED VIEW lookupcodes_silver_mv\n",
    "(\n",
    "  UniqueCode STRING,\n",
    "  Description STRING,\n",
    "  CONSTRAINT pk_lookupcodes PRIMARY KEY (UniqueCode)\n",
    ") AS\n",
    "SELECT UniqueCode, Description \n",
    "FROM lookupcodes_bronze_st\n",
    "WHERE UniqueCode IS NOT NULL AND Description IS NOT NULL;\n",
    "```\n",
    "\n",
    "This view filters out rows with null values to ensure reliable lookup data while incorporating some data modeling with the addition of a primary key\n",
    "\n",
    "---\n",
    "\n",
    "### Materialized View: Flights Silver Layer\n",
    "\n",
    "1. **Create a new query** and name it `Flights Silver_MV`.\n",
    "2. **Paste the following SQL** and make sure the catalog and schema selector are the catalog and schema that you created\n",
    "\n",
    "```sql\n",
    "\n",
    "CREATE OR REPLACE MATERIALIZED VIEW flights_silver_mv\n",
    "(\n",
    "  FlightNum,\n",
    "  Origin,\n",
    "  Dest,\n",
    "  Year,\n",
    "  Month,\n",
    "  DayofWeek,\n",
    "  Date,\n",
    "  UniqueCarrier,\n",
    "  TailNum,\n",
    "  DepTime,\n",
    "  ArrTime,\n",
    "  ActualElapsedTime,\n",
    "  Distance,\n",
    "  IsArrDelayed,\n",
    "  IsDepDelayed,\n",
    "  CONSTRAINT fk_flights_unique_carrier FOREIGN KEY (UniqueCarrier) REFERENCES <catalog>.<schema>.lookupcodes_silver_mv(UniqueCode)\n",
    ") AS\n",
    "SELECT \n",
    "  FlightNum, \n",
    "  Origin, \n",
    "  Dest, \n",
    "  Year, \n",
    "  Month, \n",
    "  DayofWeek, \n",
    "  Date, \n",
    "  UniqueCarrier, \n",
    "  TailNum, \n",
    "  DepTime, \n",
    "  ArrTime, \n",
    "  ActualElapsedTime, \n",
    "  Distance, \n",
    "  IsArrDelayed, \n",
    "  IsDepDelayed\n",
    "FROM flights_bronze_st\n",
    "WHERE FlightNum IS NOT NULL;\n",
    "```\n",
    "\n",
    "This view prepares cleansed flight data for reporting and trend analysis, while incorporating some data modeling with the addition of a foreign key\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "### Materialized View: Airports Gold Layer\n",
    "\n",
    "1. **Create a new query** and name it `Airports Gold_MV`.\n",
    "2. **Paste the following SQL** and make sure the catalog and schema selector are the catalog and schema that you created.\n",
    "\n",
    "```sql\n",
    "\n",
    "CREATE OR REPLACE MATERIALIZED VIEW airports_by_city_mv AS\n",
    "SELECT City, COUNT(*) AS number_of_airports\n",
    "FROM airports_silver_mv\n",
    "GROUP BY City;\n",
    "```\n",
    "\n",
    "This Gold layer view provides an aggregated, business-friendly summary by city.\n",
    "\n",
    "---\n",
    "\n",
    "Once these views are created, you’ll be ready to explore your cleansed and transformed data in the next steps of the lab.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "27fa25f8-f9ad-434f-bb73-5eae74a5e402",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Now that we have created all of our tables and views by writing SQL queries, these can be monitored using the Query History and Profile."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2ca2bd71-6b7d-4a7a-beb3-0dd356b97517",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 6: View Query History and Query Profile\n",
    "\n",
    "The [Query History](https://docs.databricks.com/en/sql/user/queries/query-history.html#query-history) page shows all SQL queries that have been run using SQL Warehouses. It retains query information for 30 days and is a valuable tool for monitoring, debugging, and optimizing your queries.\n",
    "\n",
    "### To access Query History:\n",
    "\n",
    "1. Navigate to the Query History tab in the left-side navigation pane.\n",
    "2. Click on any query to view its details, including:\n",
    "\n",
    "   * Execution duration\n",
    "   * SQL command\n",
    "   * Number of rows returned\n",
    "   * I/O performance\n",
    "3. Click **“See query profile”** to view detailed performance insights such as execution plan, task metrics, and stages.\n",
    "\n",
    "The [Query Profile](https://docs.databricks.com/en/sql/user/queries/query-profile.html#query-profile) is a visual interface that helps identify performance bottlenecks and optimization opportunities.\n",
    "\n",
    "### With the Query Profile, you can:\n",
    "\n",
    "* Visualize each query task and related metrics: time spent, rows processed, memory used\n",
    "* Pinpoint the slowest parts of the execution plan\n",
    "* Detect and resolve common SQL inefficiencies like full table scans or skewed joins\n",
    "\n",
    "Understanding how to read the query history and query profile is essential for monitoring your workload performance and debugging issues quickly.\n",
    "\n",
    "Let’s now move on and explore our data through the Catalog Explorer!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d33961c1-8c69-4d15-8ea2-7a8321d30107",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 7: Create AI-Generated Documentation for Data Assets\n",
    "\n",
    "As the owner of a data object, or if you have modification permissions, you can use **Catalog Explorer** to view and add [AI-generated comments](https://docs.databricks.com/en/comments/index.html#add-comments-to-data-and-ai-assets) for Unity Catalog–managed assets. These AI-generated comments help automatically document your catalogs, schemas, tables, and even table columns.\n",
    "\n",
    "Databricks uses a large language model (LLM) to generate these comments based on object metadata like column names, data types, and structure.\n",
    "\n",
    "### To add AI-generated documentation:\n",
    "\n",
    "1. Go to the Catalog Explorer tab in the left-hand navigation pane.\n",
    "2. Navigate to your `<catalog>` and click the **AI Generate** button on the right. This will auto-generate a description for the catalog.\n",
    "3. Navigate into your `<schema>` and repeat the process by clicking the **AI Generate** button to add a schema-level description.\n",
    "4. Locate your `flights_bronze_st` table, then click the **AI Generate** button on the right to generate a table description.\n",
    "5. Above the column headers of `flights_bronze_st`, click the **AI Generate** button again to generate column-level descriptions.\n",
    "6. Repeat this for all of your newly created streaming tables and views\n",
    "\n",
    "> **Note:** You can manually edit or override AI-generated comments at any time.\n",
    "\n",
    "AI-generated documentation is especially helpful for enabling data discovery and governance across your organization.\n",
    "\n",
    "With your data now documented and discoverable, you're ready to apply access controls and manage permissions across your data assets.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e0fc21b9-c372-422c-a92a-58934e409fdb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 8: Set Permissions on Data Assets\n",
    "\n",
    "Databricks makes it easy to manage data access and governance directly within **Catalog Explorer**. As a workspace admin, or any user with appropriate permissions, you can view, assign, and revoke privileges on Unity Catalog–managed objects like catalogs, schemas, volumes, tables, and more.\n",
    "\n",
    "### What You Can Do in Catalog Explorer:\n",
    "\n",
    "* Create catalogs, schemas, volumes, tables, views, etc.\n",
    "* Manage external locations and delta shares\n",
    "* View and change object ownership\n",
    "* Grant or revoke access permissions at multiple levels\n",
    "\n",
    "### In This Step:\n",
    "\n",
    "We’ll focus on how to grant and manage permissions for the following asset types:\n",
    "\n",
    "* **Catalog**\n",
    "* **Schema**\n",
    "* **Volume**\n",
    "* **Tables**\n",
    "* **Views**\n",
    "\n",
    "### How to Grant or Revoke Permissions:\n",
    "\n",
    "1. Navigate to Catalog Explorer\n",
    "2. Select the object (catalog, schema, table, or volume) you want to manage\n",
    "3. Click the **Permissions** tab\n",
    "4. Click **Grant** to assign privileges to a user \n",
    "5. Choose from a list of permissions (e.g., `SELECT`, `MODIFY`, `USAGE`, `OWN`), depending on the object type\n",
    "6. Click **Revoke** if you want to remove access\n",
    "\n",
    "> For the purpose of this lab, do **NOT** give anyone any privileges on your catalog!\n",
    "\n",
    "Setting appropriate access controls is critical to managing security, compliance, and data visibility across your organization. Now you're sure your assets are governed and accessible to the right people.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "47f27783-c951-4b93-9041-79ae0f28bce4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Conclusion\n",
    "\n",
    "Congratulations! You've successfully completed the **Admin POV** portion of this Databricks SQL hands-on lab.\n",
    "\n",
    "At this stage, you have:\n",
    "\n",
    "* Created your own catalog and schema within Unity Catalog\n",
    "* Uploaded and staged data using volumes\n",
    "* Created streaming tables to simulate real-time ingestion\n",
    "* Built materialized views across the Bronze, Silver, and Gold layers\n",
    "* Explored query history and query profiles for performance analysis\n",
    "* Used AI to auto-generate helpful documentation for your data assets\n",
    "* Applied fine-grained access controls through Catalog Explorer\n",
    "\n",
    "This foundational work prepares your environment for efficient collaboration, governance, and analytics. Your workspace users now have the structure, data, and permissions they need to begin their own analysis.\n",
    "\n",
    "Now, it's time to shift perspectives and see how the **User POV** interacts with the system you just set up.\n",
    "\n",
    "Let’s move on!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1488dea6-bc55-4d66-9bb0-4e70cc3430d0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "1"
   },
   "inputWidgetPreferences": null,
   "language": "sql",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "(Vocareum) Part 1 - Admin POV - DBSQL & BI Lab Guide DAIS 2025",
   "widgets": {}
  },
  "language_info": {
   "name": "sql"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}